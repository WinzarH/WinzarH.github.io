---
title: "LikertMaker"
subtitle: "Synthetic Likert scale data with known Mean & StDev"
author: "Hume Winzar"
affiliation: "Macquarie University"
date-format: "DD, MMM, YYYY"
date: "last-modified"
theme: "journal"
format: 
  html:
    page-layout: custom
server: shiny
---

```{r input panel}
#| panel: sidebar 

# Create a new Row in the UI for selectInputs

h4("Scale properties")
sliderInput("item_range", "Item scale range", 5,
  min = 1, max = 11, step = 1
)
# Number of items in the scale
sliderInput("items_n", "Number of items in scale", 5,
  min = 1, max = 20, step = 1
  )
# Sample size 
## (obviously we can set max sample size >300 but it can take while ...)
numericInput("sample_n", "Sample size", 30,
  min = 10, max = 300, step = 1
)

## Input first and second moments
h4("Desired scale moments")
# mean
numericInput("target_mean_scale", "Target mean", 3.75, min = 1, width = "13ch")
# standard deviation
numericInput("target_sd_scale", "Target st dev", 1.25, min = 0, width = "13ch")

actionButton("goButton", "Go!")

p("Click to start simulation")

hr()

downloadLink("downloadData", "Download data")

p("download data as csv file")

hr()

downloadLink("downloadPlot", "Download Histogram")
p("download histogram as .PNG file")

```

::: {.panel-tabset}

## data

```{r output data}
#| panel: center

shinybusy::add_busy_bar(
  color = "#69b3a2",
  centered = TRUE,
  height = "8px"
)

plotOutput("distPlot", height = 100, width = 600)

hr()

h3("Synthetic Data")

DT::dataTableOutput("mystuff")

```

## Histogram

```{r output histogram}

plotOutput("myplotout", width = "90%")

```

## Background

### Reconstructing Likert-scale data with only reported first and second moments
#### Hume Winzar, Macquarie University
Paper submitted to the ANZMAC conference, Perth,  December 2022

#### Abstract: 
Most simulations of Likert-scale data are created to test analysis methods before finalising a questionnaire. Here, however, we want to reproduce published results where only the first two moments (mean and variance) are reported. Many publications report only means and standard deviations of rating scales, often only the means. The boundaries of a scale (1-5, 1-7, etc.) will often produce skewed data, so such simple statistics can be misleading. We demonstrate a novel method to reproduce Likert-scale data using only mean and standard deviation so that they can be properly visualised and interpreted. The algorithm is surprisingly accurate. R code and an interactive Shiny app are available from the author. Such tools raise ethical concerns. 

**Keywords**: Likert Scale, Data Visualisation, Simulation, R, Optimisation 

#### Introduction 

In this study, we want to reproduce published results given only the first two moments (mean and variance). (The third and fourth moments are skewness and kurtosis respectively.) In many published reports only the means and standard deviations of rating scales are reported, often only the means. Scale boundaries (1-5, 1-7, etc.) cause most distributions to be skewed. If the standard deviation of a scale is more than the distance of the mean from the scale boundary, then most cases are closer to the boundaries than we might expect. Simple univariate statistics can be misleading, as we can see in **Figure 1**. 

#### Background 

A Likert scale is the mean, or sum, of several ordinal rating scales. They are bipolar (usually “agree-disagree”) responses to propositions that are determined to be moderately-to-highly correlated, and capturing various facets of a construct. A Likert scale that is constructed with, say, five items (questions) will have a summed range of between 5 (all rated ‘1’) and 25 (all rated ‘5’) with all integers in between, and the mean range will be ‘1’ to ‘5’ with intervals of 1/5=0.20. A Likert scale constructed from eight items will have a summed range between 8 (all rated ‘1’) and 40 (all rated ‘5’) with all integers in between, and the mean range will be ‘1’ to ‘5’ with intervals of 1/8=0.125. 

When researchers simulate Likert scales to create dummy data to check analyses ahead of finalising a questionnaire, they typically will resample with a predetermined probability distribution (Heinz, 2021). Extensions of the same probabilistic distribution idea are available for correlated data for preliminary testing of structural equation models (Grønneberg, Foldnes, & Marcoulides, 2022; Touloumis, 2016). 

Figure 1: Scale boundaries cause skewed distributions

Figure 2: Sample setup for Evolutionary algorithm in Excel Solver
 
#### Method 

Unlike the task of creating mock data, the challenge in this study is to reproduce published/reported data with a predetermined mean and standard deviation. A “probability distribution” approach is inapplicable because we cannot guarantee the desired moments. We found no examples of this simulation task in the social sciences or coding literature. We hit upon a workable solution in the Evolutionary algorithm in the Excel Solver add-in, as illustrated in **Figure 2**. In this example, we replicated a 10-item 7-point scale with mean=5.5, sd=1.0. Ten items make a summated scale of integers ranging between 10 and 70. With two criteria, mean and standard deviation, the goal is to minimise the difference between the current mean and the target mean, and between the current sd and the target sd. We amplify the differences by calculating the difference between the squares of current and target moments. The objective value to be minimised is simply the sum of the differences. This works very well in Excel but can take many minutes (hours) for a sample size in the hundreds. 

A more efficient method of achieving similar results is an evolutionary algorithm in R, Python or Julia (and, no doubt, other languages). We chose the **DEoptim** package for **R** (Mullen, Ardia, Gil, Windover, & Cline, 2011) which produces simulated data with desired properties in much less time, creating the opportunity to produce multiple data sets. With further code, we can rearrange values so that simulated Likert scales have predefined correlations. Sample code is provided on the author’s GitHub site *(reference to be added if accepted)*. 

#### Results 

**Figure 3** illustrates nine simulations for a five-point, five-item, Likert scale with a mean=2 and three runs each with standard deviations of [0.5, 1.0, 1.5]. Charts are overlaid with box plots to emphasise any skewness. We can see in runs (1, 2, 3) with mean=2, sd=0.5, that observations are symmetrically distributed. A larger sample size produces a close-to-normal distribution around the mean and median. Runs (4, 5, 6) with mean=2, sd=1.0, show positively-skewed observations, so that the median is noticeably less than the mean. Runs (7, 8, 9) with mean=2, sd=1.5, show observations are heavily skewed right, and some observations need to gather on the far end of the scale to achieve the required standard deviation. If the sd is any larger, then the optimisation algorithm may not converge. That is, the combination of moments is infeasible. In all cases the patterns are similar for any defined first and second moments. The algorithm and code used here apply to any summated rating scale with any number of items and any scale range. Simulated data are surprisingly consistent with known rating scale data. 

Figure 3: Sample runs for 5-item, 5-point Likert Scale – mean=2 stdev=[0.5, 1.0, 1.5]
 
#### Discussion

The optimisation algorithm produces simulated data with the desired mean and standard deviations accurate to at least two decimal places. When this doesn’t occur, then the combination of moments is infeasible. The purpose of this study is to simulate data like that in a published report for easier visualisation, comparison, and analysis. Additional optimisation code lets us rearrange values to produce correlated data. Such tools can be misused and raise potential ethical issues for researchers that are worthy of discussion at the conference. 


#### References 

Grønneberg, S., Foldnes, N., & Marcoulides, K. M. (2022). *covsim: An R Package for Simulating Non-Normal Data for Structural Equation Models Using Copulas*. **Journal of Statistical Software**, 102(3), 1-45. [doi:10.18637/jss.v102.i03](https://www.jstatsoft.org/article/view/v102i03)

Heinz, A. (2021). **Simulating Correlated Likert Scale Data**. BLOG post Retrieved from [https://glaswasser.github.io/simulating-correlated-likert-scale-data/](https://glaswasser.github.io/simulating-correlated-likert-scale-data/)

Mullen, K., Ardia, D., Gil, D., Windover, D., & Cline, J. (2011). *'DEoptim': An R Package for Global Optimization by Differential Evolution*. **Journal of Statistical Software**, 40(5), 1-26. [doi:10.18637/jss.v040.i06](https://www.jstatsoft.org/article/view/v040i06)

Touloumis, A. (2016). *Simulating Correlated Binary and Multinomial Responses under Marginal Model Specification: The SimCorMultRes Package*. **The R Journal**, 8(2), 79-81. Retrieved from [https://journal.r-project.org/archive/2016/RJ-2016-034/index.html](https://journal.r-project.org/archive/2016/RJ-2016-034/index.html)

:::

```{r server_function}
#| context: server

### install required packages if needed
requiredPackages <- c("dplyr", "ggplot2", "shinybusy", "DEoptim")
for (package in requiredPackages) { # Installs packages if not yet installed
  if (!requireNamespaces(package, quietly = TRUE)) {
    install.packages(package)
  }
}

### load required packages
library(dplyr)      # data manipulation
library(ggplot2)    # data visualisation
library(shinybusy)  # calculation/ progress indicator
library(DEoptim)    # non-linear optimisation

## notional output ----
output$distPlot <- renderPlot({
  xmin <- max(
    1,
    input$target_mean_scale - input$target_sd_scale
  )
  xmax <- min(
    input$target_mean_scale + input$target_sd_scale,
    input$item_range
  )
  mean_point <- data.frame(x = input$target_mean_scale, y = 0.1)
  minor.breaks <- seq(1, input$item_range, 1 / input$items_n)
  boundaries <- c(
    (1 - 1 / input$items_n),
    (input$item_range + 1 / input$items_n)
  )

  ggplot() +
    scale_x_continuous(
      breaks = c(1:input$item_range),
      minor_breaks = minor.breaks,
      limits = boundaries,
      expand = c(0, 0),
      name = "scale value"
    ) +
    geom_segment(aes(
      x = input$target_mean_scale, xend = xmax,
      y = 0.1, yend = 0.1
    ),
    colour = "blue", alpha = 1.0, size = 5
    ) +
    geom_segment(aes(
      x = xmin, xend = input$target_mean_scale,
      y = 0.1, yend = 0.1
    ),
    colour = "blue", alpha = 0.6, size = 5
    ) +
    geom_point(
      data = mean_point, aes(x = x, y = y),
      size = 6, colour = "orange"
    ) +
    labs(
      title = "expected Likert scale",
      subtitle = paste(
        "items=", input$items_n,
        ", mean=", input$target_mean_scale,
        ", sd=", input$target_sd_scale,
        ", n=", input$sample_n
      )
    ) +
    ylab("range") +
    guides(y = "none") +
    theme_classic() +
    theme_bw() +
    theme_minimal() +
    theme(
      panel.background = element_rect(
        fill = "lightblue",
        colour = "lightblue",
        size = 0.5, linetype = "solid"
      ),
      panel.grid.major = element_line(
        size = 0.5, linetype = "solid",
        colour = "white"
      ),
      panel.grid.minor = element_line(
        size = 0.25, linetype = "solid",
        colour = "white"
      )
    ) +
    theme(legend.position = "none")
})
## end notional output --

### Reaction to Go button --
mydata <- eventReactive(input$goButton, {
  ## parameters --
  scale_min <- input$items_n
  scale_max <- input$item_range * input$items_n

  target_mean <- input$target_mean_scale * input$items_n
  target_std <- input$target_sd_scale * input$items_n
  # end parameters --

  ### generate data

  ### Optimisation target ---------
  opt_scale <- function(x) {
    target_stat <- ((target_mean - mean(x)) * 200)^2 +
      ((target_std - sd(x)) * 100)^2
    target_stat
  }

  # optimisation process -------

  lower <- rep(scale_min, each = input$sample_n)
  upper <- rep(scale_max, each = input$sample_n)
  itermax <- input$sample_n * 20


  fnmap_f <- function(x) round(x)
  my_vector <- DEoptim(opt_scale, lower, upper,
    control = DEoptim.control(
      itermax = itermax,
      trace = FALSE,
      parallelType <- 1
    ),
    fnMap = fnmap_f
  )

  mydat1 <- summary(my_vector)
  mydata <- data.frame(scale = mydat1[["optim"]][["bestmem"]])
  mydata <- mydata / input$items_n

  data.structure <- paste0(
    "r", input$item_range,
    "_i", input$items_n,
    "_m", input$target_mean_scale,
    "_sd", input$target_sd_scale
  )

  mydata$structure <- data.structure

  return(mydata)
})

output$mystuff <- DT::renderDataTable(
  DT::datatable(mydata(),
    escape = FALSE,
    options = list(
      pageLength = 10, autoWidth = TRUE,
      columnDefs = list(list(targets = 2, width = "600px")),
      scrollX = TRUE
    )
  )
)


# output$mystuff <- DT::renderDataTable(DT::datatable(mydata()))

output$downloadData <- downloadHandler(
  filename = function() {
    paste0("r", input$item_range,
      "_i", input$items_n,
      "_m", input$target_mean_scale,
      "_sd", input$target_sd_scale,
      ".csv",
      sep = ""
    )
  },
  content = function(file) {
    write.csv(mydata(), file, row.names = FALSE)
  }
)


## Results plot
myplot <- reactive({
  ggplot(data = mydata(), aes(x = scale)) +
    geom_histogram(binwidth = (1 / input$items_n), fill = "#69b3a2", color = "#e9ecef", alpha = 0.9) +
    scale_x_continuous(
      breaks = c(1:input$item_range),
      limits = c((1 - 1 / input$items_n), (input$item_range + 1 / input$items_n)),
      expand = c(0, 0), name = NULL
    ) +
    labs(
      title = "Derived Likert scale distribution",
      subtitle = paste(
        "items=", input$items_n,
        ", mean=", round(mean(mydata()$scale), 2),
        ", sd=", round(sd(mydata()$scale), 2),
        ", median=", round(median(mydata()$scale), 2),
        ", n=", input$sample_n
      ),
      y = NULL,
      x = NULL
    ) +
    guides(y = "none") +
    theme_classic() +
    theme(legend.position = "none", 
          axis.text.x = element_text(size = 15))
})

  output$myplotout <- renderPlot({
    myplot()
  })


output$downloadPlot <- downloadHandler(
  filename = function() {
    paste("r", input$item_range,
      "_i", input$items_n,
      "_m", input$target_mean_scale,
      "_sd", input$target_sd_scale,
      ".png",
      sep = ""
    )
  },
  content = function(file) {
    png(file = file)
    plot(myplot())
    dev.off()
  }
)

```

## Flowchart

```{mermaid}
%%| fig-width: 7
%%| fig-height: 5

flowchart LR
  A[Response points \n 1-5, 1-7, etc] --> D(Scale Properties)
  B[n Items in scale] --> D
  E[Mean] --> G(Target Moments)
  F[St Dev] --> G
  D --> J{Data generator}
  C[Sample size] --> J
  G --> J
  J --> K[Data table]
  J --> L[Histogram]
  
```
